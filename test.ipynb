{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VmWsMd0dQbkl"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import string\n",
        "import pickle\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import concatenate, Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6B1b8m3Qbko"
      },
      "outputs": [],
      "source": [
        "# Package sentence tokenizer\n",
        "nltk.download('punkt')\n",
        "# Package lemmatization\n",
        "nltk.download('wordnet')\n",
        "# Package multilingual wordnet data\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6htp-aQwQbkq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"indonesian_conversation_data.csv\", low_memory = False, encoding='utf8')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WztiQfmCQbkq"
      },
      "outputs": [],
      "source": [
        "data = df[['Pertanyaan', 'Kategori']]\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUN8zPheQbkr"
      },
      "outputs": [],
      "source": [
        "# Removing Punctuations (Menghilangkan Punktuasi)\n",
        "data['Pertanyaan'] = data['Pertanyaan'].apply(lambda wrd:[ltrs.lower() for ltrs in wrd if ltrs not in string.punctuation])\n",
        "data['Pertanyaan'] = data['Pertanyaan'].apply(lambda wrd: ''.join(wrd))\n",
        "for i in range(data.shape[0]):\n",
        "  data['Pertanyaan'][i]=re.sub(r'\\n', ' ',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub('\\(', '',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub(r'\\)', '',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub(r',', '',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub(r'-', '',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub(r'/', '',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub(r'/', '',data['Pertanyaan'][i])\n",
        "  data['Pertanyaan'][i]=re.sub(r\"[^\\w]\", ' ',data['Pertanyaan'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "__Y_ULnqQbkr"
      },
      "outputs": [],
      "source": [
        "kata_index = {}\n",
        "for pattern in data['Pertanyaan']:\n",
        "      kata = pattern.lower().split()\n",
        "      for w in kata:\n",
        "            if w not in kata_index:\n",
        "                  kata_index[w] = len(kata_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-yBabHnjQbks"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for sentence in data['Pertanyaan']:\n",
        "    words = sentence.lower().split()  # Mengubah ke huruf kecil dan memecah kalimat menjadi kata-kata\n",
        "    sequence = [kata_index[word] for word in words]  # Mengubah setiap kata menjadi angka berdasarkan kamus\n",
        "    sequences.append(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "T-HH_crYQbkt"
      },
      "outputs": [],
      "source": [
        "# Menentukan panjang maksimum urutan angka\n",
        "max_length = max(len(sequence) for sequence in sequences)\n",
        "\n",
        "# Melakukan padding pada setiap urutan angka\n",
        "padded_sequences = []\n",
        "for sequence in sequences:\n",
        "    padded_sequence = sequence + [0] * (max_length - len(sequence))\n",
        "    padded_sequences.append(padded_sequence)\n",
        "\n",
        "# Menyimpan hasil padded dalam variabel\n",
        "padded_sequences_variable = padded_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ApDO2FiNQbku"
      },
      "outputs": [],
      "source": [
        "train = padded_sequences_variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ytMCbGkQbku"
      },
      "outputs": [],
      "source": [
        "x_train = np.array(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZNp4mpwNQbkv"
      },
      "outputs": [],
      "source": [
        "panjang_input = x_train.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "yZ_7ozRJQbkv"
      },
      "outputs": [],
      "source": [
        "labels = data['Kategori']\n",
        "label_mapping = {label: idx for idx, label in enumerate(labels.unique())}\n",
        "y_train1 = labels.map(label_mapping)\n",
        "y_train = y_train1.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kJZnPwicQbkv"
      },
      "outputs": [],
      "source": [
        "label_mapping_inverse = {idx: label for label, idx in label_mapping.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "McVKviaqQbkw"
      },
      "outputs": [],
      "source": [
        "# define vocabulary\n",
        "vocabulary = len(kata_index)\n",
        "print(\"Jumlah vocabulary data pertanyaan: \", vocabulary)\n",
        "\n",
        "# output length\n",
        "tag = np.unique(data['Kategori'])\n",
        "panjang_output = len(tag)\n",
        "print(\"Jumlah kelas unik: \", panjang_output)\n",
        "print(\"Kelas unik: \", tag[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GVhkeC-BQbkz"
      },
      "outputs": [],
      "source": [
        "jawaban = {}\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    kategori = row['Kategori']\n",
        "    jawaban_1 = row['Jawaban']\n",
        "\n",
        "    if kategori not in jawaban:\n",
        "        jawaban[kategori] = [jawaban_1]\n",
        "    else:\n",
        "        jawaban[kategori].append(jawaban_1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model('model.h5')"
      ],
      "metadata": {
        "id": "u47di1a2RAni"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miDRvX0wQbk0"
      },
      "outputs": [],
      "source": [
        "# Mengambil input dari pengguna\n",
        "prediction_input = input('üë®‚Äçü¶∞ Kamu : ')\n",
        "\n",
        "# Menghapus punktuasi dan konversi ke huruf kecil\n",
        "prediction_input = [letters.lower() for letters in prediction_input if letters not in string.punctuation]\n",
        "prediction_input = ''.join(prediction_input)\n",
        "\n",
        "# Tokenisasi input\n",
        "words = prediction_input.split()\n",
        "sequence = [kata_index.get(word, 0) for word in words]  # Mengubah kata menjadi indeks berdasarkan kamus\n",
        "padded_sequence = sequence + [0] * (panjang_input - len(sequence))  # Padding dengan menambahkan nilai 0\n",
        "\n",
        "# Membuat prediksi\n",
        "prediction = model.predict(np.array([padded_sequence]))\n",
        "predicted_class = np.argmax(prediction)\n",
        "\n",
        "# Mendapatkan tag kategori yang sesuai dengan prediksi\n",
        "predicted_tag = list(label_mapping.keys())[list(label_mapping.values()).index(predicted_class)]\n",
        "\n",
        "# Mendapatkan jawaban yang sesuai dengan tag kategori\n",
        "predicted_answer = jawaban.get(predicted_tag)[0:20]\n",
        "\n",
        "# Menampilkan hasil prediksi\n",
        "if predicted_answer is not None:\n",
        "    print(\"SMILE :) : \", random.choice(predicted_answer))\n",
        "else:\n",
        "    print(\"Jawaban tidak ditemukan.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "tf",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "cf5ab5c7f50121ffe4fcc5f1dfb71eaa97a9355dff502d18543d6465aece7a2c"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}